{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('dsconda': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8a5475c9ce6f2fec428c68161a74e331f80201abd1855f06c6f9cfa10a94328e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Basics of Deep Learning\n",
    "In this notebook, we will cover the basics behind Deep Learning. I'm talking about building a brain....\n",
    "\n",
    "![gif of some colours](https://www.fleetscience.org/sites/default/files/images/neural-mlblog.gif)\n",
    "\n",
    "Only kidding. Deep learning is a fascinating new field that has exploded over the last few years. From being used as facial recognition in apps such as SnapChat or challenger banks, to more advanced use cases such as being used in [protein-folding](https://www.independent.co.uk/life-style/gadgets-and-tech/protein-folding-ai-deepmind-google-cancer-covid-b1764008.html).\n",
    "\n",
    "In this notebook we will:\n",
    "- Explain the building blocks of neural networks\n",
    "- Go over some applications of Deep Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Building blocks of Neural Networks\n",
    "\n",
    "I have no doubt that you have heard/seen how similar neural networks are to....our brains. \n",
    "\n",
    "\n",
    "### The Perceptron\n",
    "\n",
    "The building block of neural networks. The perceptron has a rich history (covered in the background section of this book). The perceptron was created in 1958 by Frank Rosenblatt (I love that name) in Cornell, however, that story is for another day....or section in this book (backgrounds!),\n",
    "\n",
    "The perceptron is an algorithm that can learn a binary classifier (e.g. is that a cat or dog?). This is known as a threshold function, which maps an input vector *x* to an output decision *$f(x)$ = output*. Here is the formal maths to better explain my verbal fluff:\n",
    "\n",
    "$ f(x) = { 1 (if: w.x+b > 0), 0 (otherwise) $\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Gradient Descent Algo\n",
    "\n",
    "<todo> add info"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Simple Gradient Descent Implementation\n",
    "with the help from our friends over at Udacity, please view below an implementation of the Gradient Descent Algo.\n",
    "\n",
    "We begin by defining some functions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# derivation of sigmoid(x)\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "source": [
    "We begin by defining a simple neural network:\n",
    "- two input neurons: x1 and x2\n",
    "- one output neuron: y1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,5])\n",
    "y = 0.4"
   ]
  },
  {
   "source": [
    "We now define the weights, w1 and w2 for the two input neurons; x1 and x2. Also, we define a learning rate that will help us control our gradient descent step"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([-0.2,0.4])\n",
    "learnrate = 0.5"
   ]
  },
  {
   "source": [
    "we now start moving forwards through the network, known as feed forward. We can combine the input vector with the weight vector using numpy's dot product"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear combination\n",
    "# h = x[0]*weights[0] + x[1]*weights[1]\n",
    "h = np.dot(x, weights)"
   ]
  },
  {
   "source": [
    "We now apply our non-linearity, this will provide us with our output."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply non-linearity\n",
    "output = sigmoid(h)"
   ]
  },
  {
   "source": [
    "Now that we have our prediction, we are able to determine the error of our neural network. Here, we will use the difference between our actual and predicted."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = y - output"
   ]
  },
  {
   "source": [
    "The goal now is to determine how to change our weights in order to reduce the error above. This is where our good friend gradient descent and the chain rule come into play:\n",
    "- we determine the derivative of our error with respoect to our input weights. Hence:\n",
    "- change in weights = $ \\frac{d}{dw_{i}} \\frac{1}{2}{(y - \\hat{y})^2}$\n",
    "- simplifies to = learning rate * error term * $ x_{i}$\n",
    "- where:\n",
    "    - learning rate = $ n $\n",
    "    - error term = $ (y - \\hat{y}) * f'(h) $\n",
    "    - h =  $ \\sum_{i} W_{i} x_{i} $ \n",
    "\n",
    "\n",
    "We begin by calculating our f'(h)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output gradient - derivative of activation function\n",
    "output_gradient = sigmoid_prime(h)"
   ]
  },
  {
   "source": [
    "Now, we can calcualte our error term"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_trm = error * output_gradient"
   ]
  },
  {
   "source": [
    "With that, we can update our weights by combining the error term, learning rate and our x"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient desc step - updating the weights\n",
    "dsc_step = [\n",
    "    learnrate * error_trm * x[0],\n",
    "    learnrate * error_trm * x[1]\n",
    "]"
   ]
  },
  {
   "source": [
    "Which leaves..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Actual: 0.4\nNN output: 0.8581489350995123\nError: -0.45814893509951227\nWeight change: [-0.02788508381144715, -0.13942541905723577]\n"
     ]
    }
   ],
   "source": [
    "print(f'Actual: {y}')\n",
    "print(f'NN output: {output}')\n",
    "print(f'Error: {error}')\n",
    "print(f'Weight change: {dsc_step}')"
   ]
  },
  {
   "source": [
    "### More in depth..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}