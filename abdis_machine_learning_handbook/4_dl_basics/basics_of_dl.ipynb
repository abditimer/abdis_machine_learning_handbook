{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('dsconda': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8a5475c9ce6f2fec428c68161a74e331f80201abd1855f06c6f9cfa10a94328e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Basics of Deep Learning\n",
    "In this notebook, we will cover the basics behind Deep Learning. I'm talking about building a brain....\n",
    "\n",
    "![gif of some colours](https://www.fleetscience.org/sites/default/files/images/neural-mlblog.gif)\n",
    "\n",
    "Only kidding. Deep learning is a fascinating new field that has exploded over the last few years. From being used as facial recognition in apps such as SnapChat or challenger banks, to more advanced use cases such as being used in [protein-folding](https://www.independent.co.uk/life-style/gadgets-and-tech/protein-folding-ai-deepmind-google-cancer-covid-b1764008.html).\n",
    "\n",
    "In this notebook we will:\n",
    "- Explain the building blocks of neural networks\n",
    "- Go over some applications of Deep Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Building blocks of Neural Networks\n",
    "\n",
    "I have no doubt that you have heard/seen how similar neural networks are to....our brains. \n",
    "\n",
    "\n",
    "### The Perceptron\n",
    "\n",
    "The building block of neural networks. The perceptron has a rich history (covered in the background section of this book). The perceptron was created in 1958 by Frank Rosenblatt (I love that name) in Cornell, however, that story is for another day....or section in this book (backgrounds!),\n",
    "\n",
    "The perceptron is an algorithm that can learn a binary classifier (e.g. is that a cat or dog?). This is known as a threshold function, which maps an input vector *x* to an output decision *$f(x)$ = output*. Here is the formal maths to better explain my verbal fluff:\n",
    "\n",
    "$ f(x) = { 1 (if: w.x+b > 0), 0 (otherwise) $\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### The Artificial neural network\n",
    "\n",
    "Lets take a look at the high level architecture first.\n",
    "\n",
    "![3blue1brown neural network gif](https://thumbs.gfycat.com/DeadlyDeafeningAtlanticblackgoby-max-1mb.gif) \n",
    "\n",
    "The gif above of a neural network classifying images is one of the best visual ways of understanding how neural networks, work. The neural network is made up of a few key concepts:\n",
    "- An input: this is the data you pass into the network. For example, data relating to a customer (e.g. height, weight etc) or the pixels of an image\n",
    "- An output: this is the prediction of the neural network\n",
    "- A hidden layer: more on this later\n",
    "- Neuron: the network is made up of neurons, that take an input, and give an output\n",
    "\n",
    "Now, we have a slightly better understanding of what a neuron is. Lets look at a very simple neuron:\n",
    "\n",
    "![simple neural network](https://databricks.com/wp-content/uploads/2019/02/neural1.jpg) \n",
    "\n",
    "From the above image, you can clearly see the three components listed above together. \n",
    "\n",
    "### But Abdi, what is the goal of a neural network?\n",
    "\n",
    "Isn't it obvious? To me, it definitely was not when I first started to learn about neural networks. Neural networks are beautifully complex to understand, but with enough time and lots of youtube videos, you'll be able to master this topic.\n",
    "\n",
    "The goal of a neural network is to make a pretty good guess of something. For example, a phone may have a face unlock feature. The phone probably got you to take a short video/images of yourself in order to set up this security feature, and when it **learned** your face, you were then able to use it to unlock your phone. This is pretty much what we do with neural networks. We teach it by giving it data, and making sure it gets better at making predictions by adjusting the weights between neurons. More on this soon.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Gradient Descent Algo\n",
    "\n",
    "One of the best videos on neural networks, by 3Blue1Brown:\n",
    "\n",
    "<figure class=\"video_container\">\n",
    "  <iframe src=\"https://www.youtube.com/watch?v=aircAruvnKk\" frameborder=\"0\" allowfullscreen=\"true\"> </iframe>\n",
    "</figure>\n",
    "\n",
    "His series on Neural networks and Linear algebra are golden sources for learning Deep Learning."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Simple Gradient Descent Implementation\n",
    "with the help from our friends over at Udacity, please view below an implementation of the Gradient Descent Algo.\n",
    "\n",
    "We begin by defining some functions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# derivation of sigmoid(x)\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "source": [
    "We begin by defining a simple neural network:\n",
    "- two input neurons: x1 and x2\n",
    "- one output neuron: y1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,5])\n",
    "y = 0.4"
   ]
  },
  {
   "source": [
    "We now define the weights, w1 and w2 for the two input neurons; x1 and x2. Also, we define a learning rate that will help us control our gradient descent step"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([-0.2,0.4])\n",
    "learnrate = 0.5"
   ]
  },
  {
   "source": [
    "we now start moving forwards through the network, known as feed forward. We can combine the input vector with the weight vector using numpy's dot product"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear combination\n",
    "# h = x[0]*weights[0] + x[1]*weights[1]\n",
    "h = np.dot(x, weights)"
   ]
  },
  {
   "source": [
    "We now apply our non-linearity, this will provide us with our output."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply non-linearity\n",
    "output = sigmoid(h)"
   ]
  },
  {
   "source": [
    "Now that we have our prediction, we are able to determine the error of our neural network. Here, we will use the difference between our actual and predicted."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = y - output"
   ]
  },
  {
   "source": [
    "The goal now is to determine how to change our weights in order to reduce the error above. This is where our good friend gradient descent and the chain rule come into play:\n",
    "- we determine the derivative of our error with respoect to our input weights. Hence:\n",
    "- change in weights = $ \\frac{d}{dw_{i}} \\frac{1}{2}{(y - \\hat{y})^2}$\n",
    "- simplifies to = learning rate * error term * $ x_{i}$\n",
    "- where:\n",
    "    - learning rate = $ n $\n",
    "    - error term = $ (y - \\hat{y}) * f'(h) $\n",
    "    - h =  $ \\sum_{i} W_{i} x_{i} $ \n",
    "\n",
    "\n",
    "We begin by calculating our f'(h)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output gradient - derivative of activation function\n",
    "output_gradient = sigmoid_prime(h)"
   ]
  },
  {
   "source": [
    "Now, we can calcualte our error term"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_trm = error * output_gradient"
   ]
  },
  {
   "source": [
    "With that, we can update our weights by combining the error term, learning rate and our x"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient desc step - updating the weights\n",
    "dsc_step = [\n",
    "    learnrate * error_trm * x[0],\n",
    "    learnrate * error_trm * x[1]\n",
    "]"
   ]
  },
  {
   "source": [
    "Which leaves..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Actual: 0.4\nNN output: 0.8581489350995123\nError: -0.45814893509951227\nWeight change: [-0.02788508381144715, -0.13942541905723577]\n"
     ]
    }
   ],
   "source": [
    "print(f'Actual: {y}')\n",
    "print(f'NN output: {output}')\n",
    "print(f'Error: {error}')\n",
    "print(f'Weight change: {dsc_step}')"
   ]
  },
  {
   "source": [
    "### More in depth...\n",
    "\n",
    "Lets now build our own end to end example. we will begin by creating some fake data, followed by implementing our neural network."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(200,2)\n",
    "y = np.random.randint(low=0, high=2, size=(200,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_data_points, no_features = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):\n",
    "    '''Calc for sigmoid'''\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.normal(scale=1/no_features**.5, size=no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "learning_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training loss: 0.36158647930796345\n",
      "Training loss: 0.28333659424125746\n",
      "Training loss: 0.28333197496864754\n",
      "Training loss: 0.2833319747461499\n",
      "Training loss: 0.2833319747461393\n",
      "Training loss: 0.2833319747461393\n",
      "Training loss: 0.2833319747461393\n",
      "Training loss: 0.2833319747461393\n",
      "Training loss: 0.2833319747461393\n",
      "Training loss: 0.2833319747461393\n"
     ]
    }
   ],
   "source": [
    "last_loss = None\n",
    "\n",
    "for single_data_pass in range(epochs):\n",
    "    # Creating a weight change tracker\n",
    "    change_in_weights = np.zeros(weights.shape)\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        h = np.dot(x_i, weights)\n",
    "        y_hat = sigmoid(h)\n",
    "        error = y_i - y_hat\n",
    "        # error term = error * f'(h)\n",
    "        error_term = error * (y_hat * (1-y_hat))\n",
    "        # now multiply this by the current x & add to our weight update\n",
    "        change_in_weights += (error_term * x_i)\n",
    "    # now update the actual weights\n",
    "    weights += (learning_rate * change_in_weights / no_data_points)\n",
    "\n",
    "    # print the loss every 100th pass\n",
    "    if single_data_pass % (epochs/10) == 0:\n",
    "        # use current weights in NN to determine outputs\n",
    "        output = sigmoid(np.dot(x_i,weights))\n",
    "        # find the loss\n",
    "        loss = np.mean((output-y_i)**2)\n",
    "        # \n",
    "        if last_loss and last_loss < loss:\n",
    "            print(f'Train loss: {loss}, WARNING - Loss is inscreasing')\n",
    "        else:\n",
    "            print(f'Training loss: {loss}')\n",
    "        last_loss = loss "
   ]
  },
  {
   "source": [
    "## Multilayer NN\n",
    "\n",
    "Now, lets build upon our neural network, but this time, we have a hidden layer.\n",
    "\n",
    "Lets first see how to build the network to make predictions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_units = 4\n",
    "hidden_units = 3\n",
    "output_units = 2\n",
    "\n",
    "X = np.random.randn(4)\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(input_units, hidden_units))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(hidden_units, output_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.52893299 0.45833517 0.52325333]\n[0.49427808 0.46667076]\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_input = np.dot(X, weights_input_to_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "print(hidden_layer_output)\n",
    "\n",
    "output_layer_input = np.dot(hidden_layer_output, weights_hidden_to_output)\n",
    "output_layer_output = sigmoid(output_layer_input)\n",
    "print(output_layer_output)"
   ]
  },
  {
   "source": [
    "## Backpropa what?\n",
    "\n",
    "Ok, so now, how do we refine our weights? Well, this is where **backpropagation** comes in. After feeding our data forwards through the network, using feed forward, we propagate our errors backwards, making use of things such as the chain rule.\n",
    "\n",
    "Lets do an implementation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0.5, 0.2, -0.3])\n",
    "y = 0.7\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array(\n",
    "    [\n",
    "        [0.5, -0.6], [0.1, -0.2], [0.1, 0.7]\n",
    "    ]\n",
    ")\n",
    "\n",
    "weights_hidden_output = np.array([\n",
    "    0.1,-0.3\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeding data forwards through the network\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "#---\n",
    "output_layer_input = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "y_hat = sigmoid(output_layer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward propagate the errors to tune the weights\n",
    "\n",
    "# 1. calculate errors\n",
    "error = y - y_hat\n",
    "output_node_error_term = error * (y_hat * (1-y_hat))\n",
    "#----\n",
    "hidden_node_error_term = weights_hidden_output * output_node_error_term *(hidden_layer_output * (1-hidden_layer_output))\n",
    "\n",
    "# 2. calculate weight changes\n",
    "delta_w_output_node = learnrate * output_node_error_term * hidden_layer_output\n",
    "#-----\n",
    "delta_w_hidden_node = learnrate * hidden_node_error_term * x[:,None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original weights:\n[[ 0.5 -0.6]\n [ 0.1 -0.2]\n [ 0.1  0.7]]\n[ 0.1 -0.3]\n\nChange in weights for hidden layer to output layer:\n[0.01492263 0.00975438]\nChange in weights for input layer to hidden layer:\n[[ 0.00032851 -0.00092784]\n [ 0.0001314  -0.00037114]\n [-0.00019711  0.0005567 ]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Original weights:\\n{weights_input_hidden}\\n{weights_hidden_output}')\n",
    "print()\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_output_node)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_hidden_node)"
   ]
  },
  {
   "source": [
    "## Putting it all together"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "shapes (1,) and (2,) not aligned: 1 (dim 0) != 2 (dim 0)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-182463414f6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mcomplete_backprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-182463414f6a>\u001b[0m in \u001b[0;36mcomplete_backprop\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# error at hidden layer (propagated from output layer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# scale error from output layer by weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mhidden_layer_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_error_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_hidden_to_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mhidden_error_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_layer_error\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_layer_output\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mhidden_layer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,) and (2,) not aligned: 1 (dim 0) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "features = np.random.rand(200,2)\n",
    "target = np.random.randint(low=0, high=2, size=(200,1))\n",
    "\n",
    "def complete_backprop(x,y):\n",
    "    '''Complete implementation of backpropagation'''\n",
    "    n_hidden_units = 2\n",
    "    epochs = 900\n",
    "    learnrate = 0.005\n",
    "\n",
    "    n_records, n_features = features.shape\n",
    "    last_loss = None\n",
    "\n",
    "    w_input_to_hidden = np.random.normal(scale=1/n_features**.5,size=(n_features, n_hidden_units))\n",
    "    w_hidden_to_output = np.random.normal(scale=1/n_features**.5, size=n_hidden_units)\n",
    "\n",
    "    for single_epoch in range(epochs):\n",
    "        delw_input_to_hidden = np.zeros(w_input_to_hidden.shape)\n",
    "        delw_hidden_to_output = np.zeros(w_hidden_to_output.shape)\n",
    "\n",
    "        for x,y in zip(features, target):\n",
    "            # ----------------------\n",
    "            # 1. Feed data forwards\n",
    "            # ----------------------\n",
    "            \n",
    "            hidden_layer_input = np.dot(x,w_input_to_hidden)\n",
    "            hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "            output_layer_input = np.dot(hidden_layer_output, w_hidden_to_output)\n",
    "            output_layer_output = sigmoid(output_layer_input)\n",
    "\n",
    "            # ----------------------\n",
    "            # 2. Backpropagate the errors\n",
    "            # ----------------------\n",
    "\n",
    "            # error at output layer\n",
    "            prediction_error = y - output_layer_output\n",
    "            output_error_term = prediction_error * (output_layer_output * (1-output_layer_output))\n",
    "\n",
    "            # error at hidden layer (propagated from output layer)\n",
    "            # scale error from output layer by weights\n",
    "            hidden_layer_error = np.dot(output_error_term, w_hidden_to_output)\n",
    "            hidden_error_term = hidden_layer_error * (hidden_layer_output * (1-hidden_layer_output))\n",
    "\n",
    "            # ----------------------\n",
    "            # 3. Find change of weights for each data point\n",
    "            # ----------------------\n",
    "\n",
    "            delw_hidden_to_output += output_error_term * hidden_layer_output\n",
    "            delw_input_to_hidden += hidden_layer_error * x[:,None]\n",
    "        \n",
    "        \n",
    "        # Now update the actual weights\n",
    "        w_hidden_to_output += learnrate * delw_hidden_to_output / n_records\n",
    "        w_input_to_hidden += learnrate * delw_input_to_hidden / n_records\n",
    "\n",
    "        # Printing out the mean square error on the training set\n",
    "        if single_epoch % (epochs / 10) == 0:\n",
    "            hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "            out = sigmoid(np.dot(hidden_output,\n",
    "                                weights_hidden_output))\n",
    "            loss = np.mean((out - target) ** 2)\n",
    "\n",
    "            if last_loss and last_loss < loss:\n",
    "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "            else:\n",
    "                print(\"Train loss: \", loss)\n",
    "            last_loss = loss\n",
    "\n",
    "\n",
    "complete_backprop(features,target)"
   ]
  },
  {
   "source": [
    "need to work on the above..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}