{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('dsconda': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8a5475c9ce6f2fec428c68161a74e331f80201abd1855f06c6f9cfa10a94328e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Basics of Deep Learning\n",
    "In this notebook, we will cover the basics behind Deep Learning. I'm talking about building a brain....\n",
    "\n",
    "![gif of some colours](https://www.fleetscience.org/sites/default/files/images/neural-mlblog.gif)\n",
    "\n",
    "Only kidding. Deep learning is a fascinating new field that has exploded over the last few years. From being used as facial recognition in apps such as Snapchat or challenger banks, to more advanced use cases such as being used in [protein-folding](https://www.independent.co.uk/life-style/gadgets-and-tech/protein-folding-ai-deepmind-google-cancer-covid-b1764008.html).\n",
    "\n",
    "In this notebook we will:\n",
    "- Explain the building blocks of neural networks\n",
    "- Go over some applications of Deep Learning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Building blocks of Neural Networks\n",
    "\n",
    "I have no doubt that you have heard/seen how similar neural networks are to....our brains. \n",
    "\n",
    "\n",
    "### The Perceptron\n",
    "\n",
    "The building block of neural networks. The perceptron has a rich history (covered in the background section of this book). The perceptron was created in 1958 by Frank Rosenblatt (I love that name) in Cornell, however, that story is for another day....or section in this book (backgrounds!),\n",
    "\n",
    "The perceptron is an algorithm that can learn a binary classifier (e.g. is that a cat or dog?). This is known as a threshold function, which maps an input vector *x* to an output decision *$f(x)$ = output*. Here is the formal maths to better explain my verbal fluff:\n",
    "\n",
    "$ f(x) = { 1 (if: w.x+b > 0), 0 (otherwise) $\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### The Artificial neural network\n",
    "\n",
    "Lets take a look at the high level architecture first.\n",
    "\n",
    "![3blue1brown neural network gif](https://thumbs.gfycat.com/DeadlyDeafeningAtlanticblackgoby-max-1mb.gif) \n",
    "\n",
    "The gif above of a neural network classifying images is one of the best visual ways of understanding how neural networks, work. The neural network is made up of a few key concepts:\n",
    "- An input: this is the data you pass into the network. For example, data relating to a customer (e.g. height, weight etc) or the pixels of an image\n",
    "- An output: this is the prediction of the neural network\n",
    "- A hidden layer: more on this later\n",
    "- Neuron: the network is made up of neurons, that take an input, and give an output\n",
    "\n",
    "Now, we have a slightly better understanding of what a neuron is. Lets look at a very simple neuron:\n",
    "\n",
    "![simple neural network](https://databricks.com/wp-content/uploads/2019/02/neural1.jpg) \n",
    "\n",
    "From the above image, you can clearly see the three components listed above together. \n",
    "\n",
    "### But Abdi, what is the goal of a neural network?\n",
    "\n",
    "Isn't it obvious? To me, it definitely was not when I first started to learn about neural networks. Neural networks are beautifully complex to understand, but with enough time and lots of youtube videos, you'll be able to master this topic.\n",
    "\n",
    "The goal of a neural network is to make a pretty good guess of something. For example, a phone may have a face unlock feature. The phone probably got you to take a short video/images of yourself in order to set up this security feature, and when it **learned** your face, you were then able to use it to unlock your phone. This is pretty much what we do with neural networks. We teach it by giving it data, and making sure it gets better at making predictions by adjusting the weights between neurons. More on this soon.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Neural networks are made up of:\n",
    "- input layer: this is where you feed the features into the network\n",
    "- hidden layers: 1 or more layers can be built into the network, in order to build a better function approximator\n",
    "- output layer: an output layer, this can have 1 or more nodes.\n",
    "\n",
    "The above is a basic architecture. There are more that we will cover in different sections of this book."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Gradient Descent Algo\n",
    "\n",
    "One of the best videos on neural networks, by 3Blue1Brown:\n",
    "\n",
    "[![3b1b logo](https://cdn.shopify.com/s/files/1/0506/0633/collections/3blue1brown_logo_1300x.jpg?v=1528987740)](https://www.youtube.com/watch?v=aircAruvnKk)\n",
    "\n",
    "His series on Neural networks and Linear algebra are golden sources for learning Deep Learning."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Simple Gradient Descent Implementation\n",
    "with the help from our friends over at Udacity, please view below a simple implementation of a neural network. This is a very basic neural network that only has its inputs linked directly to the outputs, hence no hidden layers.\n",
    "\n",
    "We begin by defining some functions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# We will be using a sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# derivation of sigmoid(x) - will be used for backpropagating errors through the network\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "source": [
    "We begin by defining a simple neural network:\n",
    "- two input neurons: x1 and x2\n",
    "- one output neuron: y1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,5])\n",
    "y = np.array([0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of our input: (2,)\nShape of our output: (1,)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of our input: {x.shape}')\n",
    "print(f'Shape of our output: {y.shape}')"
   ]
  },
  {
   "source": [
    "We have a neuron that takes 2 inputs, and returns one output.\n",
    "\n",
    "We now define the weights, w1 and w2 for the two input neurons; x1 and x2. Also, we define a learning rate that will help us control our gradient descent step"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Weight from Node 1 to output is -0.2, and the weight from node 2 to the output is 0.4. Our learning rate is 0.5.\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([-0.2,0.4])\n",
    "learnrate = 0.5\n",
    "print(f'Weight from Node 1 to output is {weights[0]}, and the weight from node 2 to the output is {weights[1]}. Our learning rate is {learnrate}.')"
   ]
  },
  {
   "source": [
    "we now start moving forwards through the network, known as feed forward. Here, we want to take a weighted sum:\n",
    "\n",
    "$ output = Input \\cdot Weights $\n",
    "\n",
    "$ output = x_{1} \\cdot w_{1} + x_{2} \\cdot w_{2} + ... + x_{i} \\cdot w_{i}  $\n",
    "\n",
    "$ output = \\sum_{i=1}^{n} x_{i} \\cdot w_{i}  $\n",
    "\n",
    "We can combine the input vector with the weight vector using numpy's dot product"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear combination\n",
    "# h = x[0]*weights[0] + x[1]*weights[1]\n",
    "h = np.dot(x, weights)"
   ]
  },
  {
   "source": [
    "We now apply our non-linearity, this will provide us with our output."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply non-linearity\n",
    "output = sigmoid(h)"
   ]
  },
  {
   "source": [
    "Now that we have our prediction, we are able to determine the `error` of our neural network. Here, we will use the difference between our actual and predicted."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The goal now is to determine how to change our weights in order to reduce the error above. This is where our good friend gradient descent and the chain rule come into play:\n",
    "\n",
    "- we determine the derivative of our error with respect to our input weights. Hence:\n",
    "- change in weights = $ \\frac{d}{dw_{i}} \\frac{1}{2}{(y - \\hat{y})^2}$\n",
    "- simplifies to = learning rate * error term * $ x_{i}$\n",
    "- where:\n",
    "    - learning rate = $ n $\n",
    "    - error term = $ (y - \\hat{y}) * f'(h) $\n",
    "    - h =  $ \\sum_{i} W_{i} x_{i} $ \n",
    "\n",
    "\n",
    "The network we created here is simple: \n",
    "- input nodes:\n",
    "    - calculate a weighted sum\n",
    "- activation:\n",
    "    - calculate activation function e.g. sigmoid\n",
    "- cost\n",
    "    - calculate the error\n",
    "\n",
    "Hence, from the above, when we are determining the error with respect to a change in the weights, we will need to backpropagate the error. Here, we will use partial derivatives:\n",
    "\n",
    "$ \\frac{dE}{dW} = \\frac{dE}{dy} \\cdot \\frac{dy}{dh} \\cdot \\frac{dh}{dW}$\n",
    "\n",
    "Hence, putting it all together forms:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative of the error with respect to the activation\n",
    "dEdy = y - output\n",
    "# derivative of the activation with respect to weighted sum\n",
    "dydh = sigmoid_prime(h)\n",
    "# derivative of the weighted sum with respect to the weights\n",
    "dhdW = x"
   ]
  },
  {
   "source": [
    "Now, we can calcualte our error term"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent_step = learnrate * (dEdy * dydh * dhdW)"
   ]
  },
  {
   "source": [
    "Hence:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Actual output: \t\t[0.4]\nPredicted output: \t0.8581489350995123\nError: \t\t\t[-0.45814894]\n\nPrevious weights: \t[-0.2  0.4]\nUpdated weights: \t[-0.17211492  0.53942542]\nWeight change: \t\t[-0.02788508 -0.13942542]\n"
     ]
    }
   ],
   "source": [
    "print(f'Actual output: \\t\\t{y}')\n",
    "print(f'Predicted output: \\t{output}')\n",
    "print(f'Error: \\t\\t\\t{error}\\n')\n",
    "print(f'Previous weights: \\t{weights}')\n",
    "print(f'Updated weights: \\t{weights - gradient_descent_step}')\n",
    "print(f'Weight change: \\t\\t{gradient_descent_step}')"
   ]
  },
  {
   "source": [
    "### But that was a tiny network, lets go bigger\n",
    "\n",
    "We will begin by creating some fake data, followed by implementing our neural network."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For features: We have 200 instances and 2 features per instance\nFor labels: We have 200 instances and 1 label per instance\n\nlets take a look at our X\n[[0.30700922 0.90934141]\n [0.63079007 0.05063643]]\n\nLets look at our label\n[[1]\n [1]]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(200,2)\n",
    "no_data_points, no_features = x.shape\n",
    "print(f'For features: We have {no_data_points} instances and {no_features} features per instance')\n",
    "y = np.random.randint(low=0, high=2, size=(200,1))\n",
    "print(f'For labels: We have {y.shape[0]} instances and {y.shape[1]} label per instance\\n')\n",
    "\n",
    "print('lets take a look at our X')\n",
    "print(x[:2])\n",
    "print('\\nLets look at our label')\n",
    "print(y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):\n",
    "    '''Calc for sigmoid'''\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "source": [
    "Lets create the weights that connect our inputs to our output"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.normal(scale=1/no_features**.5, size=no_features)\n",
    "assert len(weights) == no_features"
   ]
  },
  {
   "source": [
    "We have now defined our neural network:\n",
    "- We have 200 data points\n",
    "- each instance has 2 features\n",
    "- with a single prediction\n",
    "- we have defined the weights between the input neurons and the output"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now, lets define our neural network with a feed forward pass. Passing our entire dataset `epoch` number of times."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training loss: 0.25692385394795986\n",
      "Training loss: 0.25692385394795986\n",
      "Training loss: 0.25692385394795986\n",
      "Training loss: 0.25692385394795986\n",
      "Training loss: 0.25692385394795986\n",
      "Training loss: 0.25692385394795986\n",
      "Training loss: 0.25692385394795986\n",
      "Training loss: 0.25692385394795986\n",
      "Training loss: 0.25692385394795986\n",
      "Training loss: 0.25692385394795986\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "learning_rate = 0.5\n",
    "last_loss = None\n",
    "\n",
    "for single_data_pass in range(epochs):\n",
    "    # reset the change_in_weights\n",
    "    change_in_weights = np.zeros(weights.shape)\n",
    "    for features, label in zip(x, y):\n",
    "        h = np.dot(features, weights)\n",
    "        pred = sigmoid(h)\n",
    "        \n",
    "        error = (label - pred)\n",
    "        # error term = error * f'(h)\n",
    "        error_term = error * (label * (1-label))\n",
    "        # now multiply this by the current x & add to our weight update\n",
    "        change_in_weights += (error_term * features)\n",
    "    # now update the actual weights after a complete pass of our data (mean of individual weight updates)\n",
    "    weights += (learning_rate * change_in_weights / no_data_points)\n",
    "\n",
    "    # print the loss every 100th pass\n",
    "    if single_data_pass % (epochs/10) == 0:\n",
    "        # use current weights in NN to determine outputs\n",
    "        output = sigmoid(np.dot(features,weights))\n",
    "        # find the loss\n",
    "        loss = np.mean((output-label)**2)\n",
    "        # \n",
    "        if last_loss and last_loss < loss:\n",
    "            print(f'Train loss: {loss}, WARNING - Loss is inscreasing')\n",
    "        else:\n",
    "            print(f'Training loss: {loss}')\n",
    "        last_loss = loss "
   ]
  },
  {
   "source": [
    "## Backpropa what?\n",
    "\n",
    "Ok, so now, how do we refine our weights? Well, this is where **backpropagation** comes in. After feeding our data forwards through the network, using feed forward, we propagate our errors backwards, making use of things such as the chain rule.\n",
    "\n",
    "Lets do an implementation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have three input nodes\n",
    "x = np.array([0.5, 0.2, -0.3])\n",
    "# one output node\n",
    "y = 0.7\n",
    "\n",
    "learnrate = 0.5\n",
    "# 2 nodes in hidden layer\n",
    "weights_input_hidden = np.array([[0.5, -0.6], [0.1, -0.2], [0.1, 0.7]])\n",
    "weights_hidden_output = np.array([0.1,-0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeding data forwards through the network\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "#---\n",
    "output_layer_input = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "y_hat = sigmoid(output_layer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward propagate the errors to tune the weights\n",
    "\n",
    "# 1. calculate errors\n",
    "error = y - y_hat\n",
    "output_node_error_term = error * (y_hat * (1-y_hat))\n",
    "#----\n",
    "hidden_node_error_term = weights_hidden_output * output_node_error_term *(hidden_layer_output * (1-hidden_layer_output))\n",
    "\n",
    "# 2. calculate weight changes\n",
    "delta_w_output_node = learnrate * output_node_error_term * hidden_layer_output\n",
    "#-----\n",
    "delta_w_hidden_node = learnrate * hidden_node_error_term * x[:,None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original weights:\n[[ 0.5 -0.6]\n [ 0.1 -0.2]\n [ 0.1  0.7]]\n[ 0.1 -0.3]\n\nChange in weights for hidden layer to output layer:\n[0.01492263 0.00975438]\nChange in weights for input layer to hidden layer:\n[[ 0.00032851 -0.00092784]\n [ 0.0001314  -0.00037114]\n [-0.00019711  0.0005567 ]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Original weights:\\n{weights_input_hidden}\\n{weights_hidden_output}')\n",
    "print()\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_output_node)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_hidden_node)"
   ]
  },
  {
   "source": [
    "## Putting it all together"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train loss:  0.2525403681187093\n",
      "Train loss:  0.2524365740756108\n",
      "Train loss:  0.25233663631306025\n",
      "Train loss:  0.2522404119715983\n",
      "Train loss:  0.2521477634680961\n",
      "Train loss:  0.25205855830450913\n",
      "Train loss:  0.2519726688830313\n",
      "Train loss:  0.251889972327498\n",
      "Train loss:  0.2518103503108861\n",
      "Train loss:  0.2517336888887526\n"
     ]
    }
   ],
   "source": [
    "features = np.random.rand(200,2)\n",
    "target = np.random.randint(low=0, high=2, size=(200,1))\n",
    "\n",
    "def complete_backprop(x,y):\n",
    "    '''Complete implementation of backpropagation'''\n",
    "    n_hidden_units = 2\n",
    "    epochs = 900\n",
    "    learnrate = 0.005\n",
    "\n",
    "    n_records, n_features = features.shape\n",
    "    last_loss = None\n",
    "\n",
    "    w_input_to_hidden = np.random.normal(scale=1/n_features**.5,size=(n_features, n_hidden_units))\n",
    "    w_hidden_to_output = np.random.normal(scale=1/n_features**.5, size=n_hidden_units)\n",
    "\n",
    "    for single_epoch in range(epochs):\n",
    "        delw_input_to_hidden = np.zeros(w_input_to_hidden.shape)\n",
    "        delw_hidden_to_output = np.zeros(w_hidden_to_output.shape)\n",
    "\n",
    "        for x,y in zip(features, target):\n",
    "            # ----------------------\n",
    "            # 1. Feed data forwards\n",
    "            # ----------------------\n",
    "            \n",
    "            hidden_layer_input = np.dot(x,w_input_to_hidden)\n",
    "            hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "            output_layer_input = np.dot(hidden_layer_output, w_hidden_to_output)\n",
    "            output_layer_output = sigmoid(output_layer_input)\n",
    "\n",
    "            # ----------------------\n",
    "            # 2. Backpropagate the errors\n",
    "            # ----------------------\n",
    "\n",
    "            # error at output layer\n",
    "            prediction_error = y - output_layer_output\n",
    "            output_error_term = prediction_error * (output_layer_output * (1-output_layer_output))\n",
    "\n",
    "            # error at hidden layer (propagated from output layer)\n",
    "            # scale error from output layer by weights\n",
    "            hidden_layer_error = np.multiply(output_error_term, w_hidden_to_output)\n",
    "            hidden_error_term = hidden_layer_error * (hidden_layer_output * (1-hidden_layer_output))\n",
    "\n",
    "            # ----------------------\n",
    "            # 3. Find change of weights for each data point\n",
    "            # ----------------------\n",
    "\n",
    "            delw_hidden_to_output += output_error_term * hidden_layer_output\n",
    "            delw_input_to_hidden += hidden_error_term * x[:,None]\n",
    "        \n",
    "        \n",
    "        # Now update the actual weights\n",
    "        w_hidden_to_output += learnrate * delw_hidden_to_output / n_records\n",
    "        w_input_to_hidden += learnrate * delw_input_to_hidden / n_records\n",
    "\n",
    "        # Printing out the mean square error on the training set\n",
    "        if single_epoch % (epochs / 10) == 0:\n",
    "            hidden_output = sigmoid(np.dot(x, w_input_to_hidden))\n",
    "            out = sigmoid(np.dot(hidden_output,\n",
    "                                w_hidden_to_output))\n",
    "            loss = np.mean((out - target) ** 2)\n",
    "\n",
    "            if last_loss and last_loss < loss:\n",
    "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "            else:\n",
    "                print(\"Train loss: \", loss)\n",
    "            last_loss = loss\n",
    "\n",
    "\n",
    "complete_backprop(features,target)"
   ]
  }
 ]
}