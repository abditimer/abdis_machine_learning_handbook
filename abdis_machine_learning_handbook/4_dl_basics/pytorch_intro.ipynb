{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37164bitdatascienceenvvenvc7e335f45fcf4ed5bb97715a7b64d407",
   "display_name": "Python 3.7.1 64-bit ('datascienceenv': venv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Pytorch & Deep Learning\n",
    "\n",
    "![pytorch image](https://miro.medium.com/max/919/1*Z4L6D1RiQauGmB3TGK_wJg.gif)\n",
    "\n",
    "Pytorch was released in 2017 as an open source project by Facebook's AI Research team. It is a framework that is extremely popular. It is a framework for building and training neural nets. Pytorch takes tensors, and simplifies the move to GPUs for faster processing needed for training neural networks. \n",
    "\n",
    "Pytorch also provides a much loved module that enables automatic calcs for gradients (for backprop), so no more having to calculate lots of partial derivatives. Yay!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Pytorch Neural Nets and Tensors\n",
    "\n",
    "For recap, we calculate the output of a network by:\n",
    "\n",
    "$ y = f( \\sum_{i} w_{i}x_{i} + b ) $\n",
    "\n",
    "### But what are tensors?\n",
    "\n",
    "You can think of neural network calcs as a bunch of linear algebra calcs on tensors. Tensors, are generalised formats of matrics:\n",
    "- 1D Tensor -> vector\n",
    "- 2D Tensor -> Matrix\n",
    "- 3D tensor -> Array\n",
    "\n",
    "Tensors are fundemental **data structures**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Lets build a simple Neural Network using Pytorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import torch \n",
    "torch.__version__"
   ]
  },
  {
   "source": [
    "We will begin by creating a function for our activation function, followed by creating the structures for our features, weights and bias. We will use `torch.randn`, which fills our sized tensor with values from a normal distribution with 0 mean and 1 standard deviation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "features = torch.randn(size=(1,5))\n",
    "weights  = torch.rand_like(features)\n",
    "bias = torch.randn(size=(1,1))"
   ]
  },
  {
   "source": [
    "Now, lets do a simple feed forward pass, using the equation for determining $y$ detailed above. The first step is to always calculate the dot product between our features (input layer) and our weights."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.6140]])\n"
     ]
    }
   ],
   "source": [
    "# summ = 0\n",
    "# for i in range(features.shape[1]):\n",
    "#     summ += (features[0][i]* weights[0][i])\n",
    "# summ = summ + bias\n",
    "# summ = sigmoid(summ)\n",
    "# print(summ)\n",
    "\n",
    "y = sigmoid((torch.sum(features*weights) + bias))\n",
    "print(y)"
   ]
  },
  {
   "source": [
    "Lets instead calculate using an in-built function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x5 and 1x5)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8f55c3abfe5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x5 and 1x5)"
     ]
    }
   ],
   "source": [
    "torch.mm(features, weights)"
   ]
  },
  {
   "source": [
    "As you'd expect, we are having a shape mismatch. This is because in order to multiply two matrices, we need them to be in theright shapes (no of cols of the first tensor must be the same as the no of rows of tensor 2). There are three solutions to this problem:\n",
    "- `matr.reshape(a,b)`: returns a new tensor with the same data as matr, but with size (a,b). Sometimes it clones it to a new part in memory however\n",
    "- `matr.reize_(a,b)`: returns same tensor with new shape. But, if new shape is smaller, it removes elements from tensor and if it is bigger, those new elements will be uninitialised. the underscore shows it will happen in-place\n",
    "- `matr.view(a,b)`: returns a new tensor with the same data as matr, but with the new size (a,b)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.6140]])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "y = sigmoid(torch.mm(features,weights.view(5,1)) + bias)\n",
    "y"
   ]
  },
  {
   "source": [
    "Now, we will enhance our network by adding a hidden layer. Now, with a hidden layer, this will add some very tiny little small complexity, but luckily linear algebtra and pytorch abstract away any mathematical complexity.\n",
    "\n",
    "Our neural network will have:\n",
    "- input layer size (1,5) -> features\n",
    "- hidden layer -> 2 nodes\n",
    "- output layer -> 1 node \n",
    "\n",
    "Hence we will calculate:\n",
    "\n",
    "$ y = f_{2}( f_{1}( x^{->}W_{1} ) W_{2} ) $"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Our feature input:  tensor([[ 1.2026, -0.0063, -0.2413]])\nWe have 3 input nodes, 2 hidden nodes, and 1 output nodes.\n"
     ]
    }
   ],
   "source": [
    "features = torch.randn(size=(1,3))\n",
    "print('Our feature input: ', features)\n",
    "# define network\n",
    "input_nodes = features.shape[1] #3\n",
    "hidden_nodes = 2\n",
    "output_nodes = 1\n",
    "print(f'We have {input_nodes} input nodes, {hidden_nodes} hidden nodes, and {output_nodes} output nodes.')\n",
    "# define weights\n",
    "w_0_1 = torch.randn(size=(input_nodes, hidden_nodes))\n",
    "w_1_2 = torch.randn(size=(hidden_nodes, output_nodes))\n",
    "# define bias\n",
    "b_0_1 = torch.randn(size=(1,hidden_nodes))\n",
    "b_1_2 = torch.randn(size=(1, output_nodes))"
   ]
  },
  {
   "source": [
    "A very simple network output can be calculated by:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.6247]])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "h1 = sigmoid(torch.mm(features, w_0_1) + b_0_1)\n",
    "y = sigmoid(torch.mm(h1, w_1_2) + b_1_2)\n",
    "y"
   ]
  },
  {
   "source": [
    "## Man, is this really how you can build large networks?\n",
    "I know, you're probably thinking this is such a tedious way to build neural networks. If we are doing linear algebra per layer, what happens when we start building neural networks with 100s of layers!\n",
    "\n",
    "Well, do not worry. Pytorch has a great framework that provides an easy way to build large neural networks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt "
   ]
  }
 ]
}