{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# PySpark refresher\n",
    "\n",
    "![spark image](https://databricks.com/wp-content/uploads/2019/02/largest-open-source-apache-spark.png)\n",
    "\n",
    "This notebook will introduce you to the topic of pyspark.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "5+5"
   ]
  },
  {
   "source": [
    "## Background\n",
    "\n",
    "### What is spark and why should I care?\n",
    "\n",
    "At a high-level, spark is:\n",
    "- open-source distributed querting and processing  engine. It is written in Scala and Java, running in JVM\n",
    "- it is very fast (compared to hadoop)\n",
    "- API access via Scala, python and more\n",
    "\n",
    "Spark, at a high level, can be viewed as:\n",
    "\n",
    "![spark ecosystem overview](https://docs.snowflake.com/en/_images/spark-snowflake-data-source.png)\n",
    "\n",
    "From the image above, you can see that spark is made up of a core API that abstracts the scala and java code. It is made up of:\n",
    "- spark SQL: more efficient way of interacting with data \n",
    "- spark streaming, mllib, graphx: streaming, graph and ML frameworks\n",
    "\n",
    "### But how does spark work?\n",
    "\n",
    "You can think of spark of being made up of `Jobs`. These jobs must have atleast one `Driver` and one `Worker`: A Driver plans all the execution of the jobs and commands the workers to carry out a specific `task` from the job. A worker does not decide what task they want to run, instead, this is commanded by the driver. Hence, we can summarise a spark job as a optimised set of tasks that are processed in order. Spark is able to optimise this by identifying which tasks can be run in parallel. This can also be viewed by the generated Directed Acyclic Graph (DAG) that is created by spark."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}